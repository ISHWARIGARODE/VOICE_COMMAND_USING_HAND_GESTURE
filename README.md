Project Description:

This project combines computer vision and speech synthesis to control voice commands through hand gestures using Python. The system detects specific hand gestures captured by a webcam and converts them into corresponding voice outputs using a text-to-speech engine.

It leverages MediaPipe for real-time hand tracking and gesture recognition, and pyttsx3 or similar libraries for generating speech. The main goal is to create an accessible, touchless interface, which can be useful for people with disabilities, smart home control, or human-computer interaction.
